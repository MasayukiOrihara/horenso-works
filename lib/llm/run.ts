import { Runnable } from "@langchain/core/runnables";
import { Document } from "langchain/document";
import { OpenAi4_1Mini, OpenAi4oMini } from "./models";
import { UNKNOWN_ERROR } from "../message/error";
import { PromptTemplate } from "@langchain/core/prompts";
import { saveLlmLog } from "../supabase/services/saveLlmLog.service";
import { extractOutputText } from "../utils";
import * as TYPE from "../type";

// å‹ã‚¬ãƒ¼ãƒ‰é–¢æ•°
function isLLMEndPayload(x: unknown): x is TYPE.LLMEndPayload {
  return typeof x === "object" && x !== null;
}

// LLMçµæœã®åŸºæœ¬å‹ã‚’å®šç¾©
export type LLMParserResult =
  | string
  | string[]
  | Document<TYPE.PhrasesMetadata>[];
// ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ‰±ã†ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
export interface StreamChunk {
  content?: string;
  additional_kwargs?: Record<string, unknown>;
}

// Streamçµæœã¨Invokeçµæœã®ãƒ¦ãƒ‹ã‚ªãƒ³å‹
type LLMResponse = LLMParserResult | AsyncIterable<StreamChunk>;

// ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯èƒ½ãªLLMä¸€è¦§
const fallbackLLMs: Runnable[] = [OpenAi4_1Mini, OpenAi4oMini];

/* ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ãŸã¨ãã«åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆã‚‹å¯¾ç­– + æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ä»˜ã */
export async function runWithFallback(
  runnable: Runnable,
  input: Record<string, unknown>,
  options?: TYPE.RunWithFallbackOptions
): Promise<LLMResponse | ReadableStream<StreamChunk>> {
  // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
  const {
    mode = "invoke",
    parser,
    maxRetries = 3,
    baseDelay = 200,
    label = "",
    sessionId = "",
    onStreamEnd = async () => {},
  } = options || {};

  for (const model of fallbackLLMs) {
    for (let retry = 0; retry < maxRetries; retry++) {
      try {
        // LLM å‘¼ã³å‡ºã—
        let pipeline = runnable.pipe(model);
        if (parser) {
          pipeline = pipeline.pipe(parser);
        }

        const callback = createLatencyCallback(
          label ? label : model.lc_kwargs.model
        );

        const result: LLMResponse =
          mode === "stream"
            ? await pipeline.stream(input, {
                callbacks: [callback],
              })
            : await pipeline.invoke(input, {
                callbacks: [callback],
              });

        // âœ… æˆåŠŸãƒ¢ãƒ‡ãƒ«ã®ãƒ­ã‚°
        console.log(`[LLM] Using model: ${model.lc_kwargs.model}`);

        // å®Œæˆã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å–å¾—
        const fullPrompt = await getFullPrompt(runnable, input);
        const payload: TYPE.LLMPayload = {
          label: label,
          llmName: model.lc_kwargs.model,
          sessionId: sessionId,
          fullPrompt: fullPrompt,
        };

        console.log("ğŸ¶");
        console.log(result);
        console.log(typeof result);
        console.log("ğŸˆ");

        // stream å¿œç­”æ™‚çµ‚äº†å¾Œã«å‡¦ç†ã‚’è¡Œã†
        return mode === "stream"
          ? enhancedStream(
              result as AsyncIterable<StreamChunk>,
              payload,
              callback,
              onStreamEnd
            )
          : await enhancedInvoke(result as LLMParserResult, payload, callback);
      } catch (err) {
        const message = err instanceof Error ? err.message : UNKNOWN_ERROR;
        const isRateLimited =
          message.includes("429") ||
          message.includes("rate limit") ||
          message.includes("overloaded");
        if (!isRateLimited) throw err;

        // æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã®å‡¦ç†
        const delay = Math.min(baseDelay * 2 ** retry, 5000); // æœ€å¤§5ç§’
        const jitter = Math.random() * 100;
        console.warn(
          `Model ${model.lc_kwargs.model} failed with rate limit (${
            retry + 1
          }/${maxRetries}): ${message}`
        );
        await new Promise((res) => setTimeout(res, delay + jitter));
      }
    }
    // æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ¬¡ã®ãƒ«ãƒ¼ãƒ—ã¸ï¼‰
    console.warn(
      `Model ${model.lc_kwargs.model} failed all retries. Trying next model.`
    );
  }
  // ã©ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚‚æˆåŠŸã—ãªã‹ã£ãŸå ´åˆ
  throw new Error("All fallback models failed.");
}

/* ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨æ–‡å–å¾— */
const getFullPrompt = async (
  runnable: Runnable,
  input: Record<string, unknown>
): Promise<string> => {
  const fullPrompt = await (runnable as PromptTemplate).format(input);
  return fullPrompt;
};

const enhancedInvoke = async (
  result: LLMParserResult,
  payload: TYPE.LLMPayload,
  callback: ReturnType<typeof createLatencyCallback>
): Promise<LLMParserResult> => {
  const outputText = extractOutputText(result);

  // invoke ã¯ã“ã“ã§ onLLMEnd ãŒæ¸ˆã‚“ã§ã‚‹
  const metrics = callback.getMetrics();
  const usage = callback.getUsage();

  try {
    await saveLlmLog({
      sessionId: payload.sessionId,
      label: payload.label,
      llmName: payload.llmName,
      fullPrompt: payload.fullPrompt,
      fullOutput: outputText,
      usage,
      metrics,
    });
  } catch (e) {
    console.error("[llm_logs.save invoke] insert failed:", e);
  }

  return result;
};

/* ã‚¹ãƒˆãƒªãƒ¼ãƒ çµ‚äº†å¾Œã®å‡¦ç† */
const enhancedStream = (
  stream: AsyncIterable<StreamChunk>,
  payload: TYPE.LLMPayload,
  callback: ReturnType<typeof createLatencyCallback>,
  onStreamEnd?: (response: string) => Promise<void>
) =>
  new ReadableStream({
    async start(controller) {
      let response = "";

      for await (const chunk of stream) {
        response += chunk.content || "";
        if (typeof chunk.content === "string") {
          controller.enqueue(chunk.content);
        } else if (chunk.additional_kwargs) {
          // å¿…è¦ãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
          controller.enqueue(JSON.stringify(chunk.additional_kwargs));
        }
        // ä½•ã‚‚ãªã‘ã‚Œã° enqueue ã—ãªã„ï¼ˆç„¡éŸ³ï¼‰
      }

      // çµ‚äº†æ™‚ã«å¤–éƒ¨å‡¦ç†ã‚’èµ°ã‚‰ã›ã‚‹
      if (onStreamEnd) {
        await onStreamEnd(response);

        // æƒ…å ±ã‚’å¤–éƒ¨ä¿å­˜
        const metrics = callback.getMetrics();
        const usage = callback.getUsage();

        // fullOutput ã¯ stream ã§æºœã‚ãŸ response
        try {
          await saveLlmLog({
            sessionId: payload.sessionId,
            label: payload.label,
            llmName: payload.llmName,
            fullPrompt: payload.fullPrompt,
            fullOutput: response,
            usage,
            metrics,
          });
        } catch (e) {
          console.error("[llm_logs.save stream] insert failed:", e);
        }
      }
      controller.close();
    },
  });

/* å‘¼ã³å‡ºã—æ™‚é–“æ¸¬å®šç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ï¼ˆãƒ©ãƒ™ãƒ«ä»˜ãï¼‰ */
const createLatencyCallback = (label: string) => {
  let startTime = 0;
  let firstTokenTime: number | null = null;
  let finishedAt: number | null = null;
  let usage: TYPE.Usage | null = null;

  const metrics: TYPE.LatencyMetrics = {
    label,
    startedAt: Date.now(),
  };

  return {
    // LLM å‘¼ã³å‡ºã—ç›´å¾Œ
    handleLLMStart(): void {
      startTime = Date.now();
      metrics.startedAt = startTime;
    },

    // æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³
    handleLLMNewToken(): void {
      if (firstTokenTime === null) {
        firstTokenTime = Date.now() - startTime;
        metrics.firstTokenMs = firstTokenTime;

        const seconds = Math.floor(firstTokenTime / 1000);
        const milliseconds = firstTokenTime % 1000;

        // ãƒ­ã‚°ã«å‡ºåŠ›
        console.log(
          `[${label}] first token latency: ${seconds}s ${milliseconds}ms`
        );
      }
    },

    // å‡ºåŠ›å®Œäº†
    handleLLMEnd(payload?: unknown): void {
      // è¨ˆæ¸¬ç§’æ•°ã‚’è¨ˆç®—
      finishedAt = Date.now();
      const elapsedMs = finishedAt - startTime;
      metrics.finishedAt = finishedAt;
      metrics.totalMs = elapsedMs;

      const seconds = Math.floor(elapsedMs / 1000);
      const milliseconds = elapsedMs % 1000;

      // ãƒ­ã‚°ã«å‡ºåŠ›
      console.log(`[${label}] all latency: ${seconds}s ${milliseconds}ms`);

      // usageã‚’å¤šæ®µãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å–å¾—
      if (isLLMEndPayload(payload)) {
        const u =
          payload.llmOutput?.tokenUsage ||
          payload.usage ||
          payload.response?.usage ||
          null;

        if (u) {
          usage = {
            prompt: u.promptTokens ?? u.prompt_tokens ?? u.input_tokens ?? 0,
            completion:
              u.completionTokens ?? u.completion_tokens ?? u.output_tokens ?? 0,
            total: u.totalTokens ?? u.total_tokens ?? undefined,
          };
        }
      }
    },

    // --- è¿½åŠ : å¤–ã‹ã‚‰è¨ˆæ¸¬çµæœã‚’å–å¾—ã§ãã‚‹ã‚ˆã†ã« ---
    getMetrics(): TYPE.LatencyMetrics {
      return { ...metrics };
    },

    getUsage(): TYPE.Usage | null {
      return usage;
    },
  };
};
