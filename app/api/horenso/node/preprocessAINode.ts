import { Document } from "langchain/document";
import { BaseMessage } from "@langchain/core/messages";

import * as MSG from "../contents/messages";
import { HorensoMetadata, QADocumentMetadata, QAEntry } from "@/lib/type";
import { embeddings } from "@/lib/llm/models";

import {
  notCrrectFilePath,
  qaEntriesFilePath,
  semanticFilePath,
} from "@/lib/path";
import { splitInputLlm } from "../lib/llm/splitInput";
import { generateHintLlm } from "../lib/llm/generateHint";
import { sortScore } from "../lib/match/score";
import { cachedVectorStore } from "../lib/match/vectorStore";
import { messageToText } from "../lib/utils";
import { pushLog } from "../lib/log/logBuffer";
import { readJson } from "@/lib/file/read";
import { requestApi } from "@/lib/api/request";
import { RunnableParallel } from "@langchain/core/runnables";
import { buildQADocuments } from "../lib/entry";
import { analyzeInput } from "../lib/llm/analyzeInput";

// å®šæ•°
const MATCH_VALIDATE = "/api/horenso/lib/match/validate";
const MATCH_PATH = "/api/horenso/lib/match";

type AiNode = {
  messages: BaseMessage[];
  step: number;
  baseUrl: string;
  whoUseDocuments: Document<HorensoMetadata>[];
  whyUseDocuments: Document<HorensoMetadata>[];
};

/**
 * LLMã§ã®å‡¦ç†ã‚’ã¾ã¨ã‚ã¦äº‹å‰ã«å®Ÿè¡Œã™ã‚‹ãƒãƒ¼ãƒ‰
 * @param param0
 * @returns
 */
export async function preprocessAiNode({
  messages,
  step,
  baseUrl,
  whoUseDocuments,
  whyUseDocuments,
}: AiNode) {
  /* â“ª ä½¿ã†å¤‰æ•°ã®æº–å‚™  */
  pushLog("ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ä¸­ã§ã™...");
  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç­”ãˆ
  const userMessage = messageToText(messages, messages.length - 1);
  // æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆãªã‘ã‚Œã°ç©ºé…åˆ—ï¼‰
  const qaList: QAEntry[] = readJson(qaEntriesFilePath());
  // åŸ‹ã‚è¾¼ã¿ä½œæˆç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ—
  const qaDocuments = buildQADocuments(qaList, step);
  // ã‚ã„ã¾ã„å›ç­”jsonã®èª­ã¿è¾¼ã¿
  const semanticList = readJson(semanticFilePath());
  const notCorrectList = readJson(notCrrectFilePath());
  // å›ç­”ãƒã‚§ãƒƒã‚¯åˆ¤å®šã‚’å–å¾—
  const readShouldValidate = await requestApi(baseUrl, MATCH_VALIDATE, {
    method: "GET",
  });

  // ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
  let sepKeywordPrompt = "";
  let useDocuments: Document<HorensoMetadata>[] = [];
  let k = 1;
  let allTrue = false;
  let shouldValidate = false;
  let question = "";
  switch (step) {
    case 0:
      sepKeywordPrompt = MSG.KEYWORD_EXTRACTION_PROMPT;
      useDocuments = whoUseDocuments;
      question = MSG.FOR_REPORT_COMMUNICATION;
      shouldValidate = readShouldValidate.who ?? false;
      break;
    case 1:
      sepKeywordPrompt = MSG.CLAIM_EXTRACTION_PROMPT;
      useDocuments = whyUseDocuments;
      k = 3;
      allTrue = true;
      question = MSG.REPORT_REASON_FOR_LEADER;
      shouldValidate = readShouldValidate.why ?? true;
      break;
  }

  /* â‘  ç­”ãˆã®åˆ†é›¢ ã¨ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‚’åŸ‹ã‚è¾¼ã¿ ã¨ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆ */
  pushLog("å›ç­”ã®ç¢ºèªä¸­ã§ã™...");
  // å…¥åŠ›ã®åˆ†æ
  const analyzeResultPromise = analyzeInput(userMessage, question);
  const [userAnswer, userEmbedding, vectorStore] = await Promise.all([
    splitInputLlm(sepKeywordPrompt, userMessage),
    embeddings.embedQuery(userMessage),
    cachedVectorStore(qaDocuments),
  ]);
  console.log("è³ªå•ã®åˆ†é›¢ã—ãŸç­”ãˆ: ");
  console.log(userAnswer);
  console.log(" --- ");

  /* â‘¡ æ­£è§£ãƒã‚§ãƒƒã‚¯(OpenAiåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨) ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æº–å‚™ + æ¯”è¼ƒ */
  pushLog("æ­£è§£ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã£ã¦ã„ã¾ã™...");
  // langchain ã®ä¸¦åˆ—å‡¦ç†ã‚’åˆ©ç”¨
  const steps: Record<string, () => Promise<unknown>> = {};
  userAnswer.forEach((answer, i) => {
    steps[`checkAnswer_${i}`] = async () =>
      requestApi(baseUrl, MATCH_PATH, {
        method: "POST",
        body: {
          matchAnswerArgs: {
            userAnswer: answer,
            documents: useDocuments,
            topK: k,
            allTrue,
            shouldValidate,
            semanticList,
            notCorrectList,
          },
        },
      });
  });
  const checkUserAnswers = new RunnableParallel({ steps });

  //vectorStoreæ¤œç´¢ã¨ä¸¦åˆ—ã«å®Ÿè¡Œ(å…¨ä½“ã®å‡¦ç†æ™‚é–“ã‚‚è¨ˆæ¸¬)
  const start = Date.now();
  const [matchResultsMap, rawQaEmbeddings] = await Promise.all([
    checkUserAnswers.invoke([]), // RunnableParallel å®Ÿè¡Œ
    vectorStore.similaritySearchVectorWithScore(userEmbedding, 5),
  ]);
  const end = Date.now();
  const matchResults = Object.values(matchResultsMap);
  const evaluationData = matchResults.map((r) => r.evaluationData).flat();

  console.log("ğŸ¶");
  console.log(evaluationData);

  console.log("\n");
  console.log(`å‡¦ç†æ™‚é–“(ms): ${end - start} ms`);
  console.log(`OpenAI Embeddings ãƒã‚§ãƒƒã‚¯å®Œäº† \n ---`);

  /* â‘¢ ãƒ’ãƒ³ãƒˆã®å–å¾—ï¼ˆæ­£è§£ã—ã¦ã„ãŸã¨ãã¯é£›ã°ã™ï¼‰ */
  pushLog("ãƒ’ãƒ³ãƒˆã®æº–å‚™ä¸­ã§ã™...");
  const tempIsCorrect = false; // æ­£è§£åˆ¤å®šã§é£›ã°ã™ï¼ˆâ€»â€» å¾Œã§è€ƒãˆã‚‹ï¼‰
  let qaEmbeddings: [Document<QADocumentMetadata>, number][] = [];
  let getHint: string = "";
  if (!tempIsCorrect) {
    const sortData = sortScore(evaluationData);
    const getHintPromises = generateHintLlm(question, sortData, useDocuments);

    qaEmbeddings = rawQaEmbeddings as [Document<QADocumentMetadata>, number][];
    getHint = await getHintPromises;
    console.log("è³ªå•1ã®ãƒ’ãƒ³ãƒˆ: \n" + getHint);
  }

  const analyzeResult = await analyzeResultPromise;
  console.log(analyzeResult);
  pushLog("è¿”ç­”ã®ç”Ÿæˆä¸­ã§ã™...");
  return { evaluationData, qaEmbeddings, getHint, analyzeResult };
}
