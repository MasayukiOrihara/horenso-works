import { RemoveMessage } from "@langchain/core/messages";
import {
  Annotation,
  MemorySaver,
  MessagesAnnotation,
  StateGraph,
} from "@langchain/langgraph";

import { OpenAi4_1Mini } from "@/lib/llm/models";
import { MEMORY_SUMMARY_PROMPT, MEMORY_UPDATE_PROMPT } from "./contents";
import { SESSIONID_ERROR, UNKNOWN_ERROR } from "@/lib/message/messages";
import { convertToOpenAIFormat } from "./utils";

/** ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŒ¿å…¥ã™ã‚‹å‡¦ç† */
async function insartMessages(state: typeof GraphAnnotation.State) {
  const messages = state.messages;

  return { messages: messages };
}

/** ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹å‡¦ç† */
async function convertFormat(state: typeof GraphAnnotation.State) {
  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å–å¾—
  const messages = state.messages;
  const formatted = state.formatted ?? [];

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é¸æŠ
  const len = state.messages.length;
  const previousMessage = state.messages.slice(Math.max(0, len - 2), len);

  // message ã‚’æ•´å½¢
  for (const message of previousMessage) {
    const openaiFormat = convertToOpenAIFormat(message);
    const stringFormat = `${openaiFormat.role}: ${openaiFormat.content}`;
    const cleanFormat = stringFormat.replace(/[\r\n]+/g, "");
    formatted.push(cleanFormat);
  }

  // è¦ç´„ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é™¤å»
  const deleteMessages = messages
    .slice(0, -2)
    .map((m) => new RemoveMessage({ id: m.id! }));

  return { formatted: formatted, messages: deleteMessages };
}

/** ä¼šè©±ã‚’è¡Œã†ã‹è¦ç´„ã™ã‚‹ã‹ã®åˆ¤æ–­å‡¦ç† */
async function shouldContenue(state: typeof GraphAnnotation.State) {
  const formatted = state.formatted;

  if (formatted.length > 6) return "summarize";
  return "__end__";
}

/** ä¼šè©±ã®è¦ç´„å‡¦ç† */
async function summarizeConversation(state: typeof GraphAnnotation.State) {
  const summary = state.summary;
  const formatted = state.formatted;

  let summaryMessage;

  if (summary) {
    summaryMessage = MEMORY_UPDATE_PROMPT.replace("{summary}", summary);
  } else {
    summaryMessage = MEMORY_SUMMARY_PROMPT;
  }

  // è¦ç´„å‡¦ç†
  const context = `${formatted.join("\n")}\n${summaryMessage}`;
  const response = await OpenAi4_1Mini.invoke(context);

  // è¦ç´„ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é™¤å»
  const empty: string[] = [];
  return { summary: response.content, formatted: empty };
}

/** è¦ç´„ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹å‡¦ç† */
async function prepareMessages(state: typeof GraphAnnotation.State) {
  const formatted = state.formatted ?? [];

  const summary = state.summary;
  // è¦ç´„ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¿½åŠ 
  const systemMessage = `Previous conversation summary: ${summary}`;
  formatted.push(systemMessage);

  return { formatted: formatted };
}

// ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è¿½åŠ 
const GraphAnnotation = Annotation.Root({
  summary: Annotation<string>(),
  formatted: Annotation<string[]>(),
  sessionId: Annotation<string>(),
  ...MessagesAnnotation.spec,
});

// ã‚°ãƒ©ãƒ•
const workflow = new StateGraph(GraphAnnotation)
  // ãƒãƒ¼ãƒ‰è¿½åŠ 
  .addNode("insart", insartMessages)
  .addNode("format", convertFormat)
  .addNode("prepare", prepareMessages)
  .addNode("summarize", summarizeConversation)

  // ã‚¨ãƒƒã‚¸è¿½åŠ 
  .addEdge("__start__", "insart")
  .addEdge("insart", "format")
  .addConditionalEdges("format", shouldContenue)
  .addEdge("summarize", "prepare")
  .addEdge("prepare", "__end__");

// è¨˜æ†¶ã®è¿½åŠ 
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });

/**
 * ä¼šè©±å±¥æ­´è¦ç´„API
 * ã‚ªãƒ³ãƒ¡ãƒ¢ãƒªç‰ˆæœ€æ‚ªã®æ‰‹æ®µã¨ã—ã¦ä¸€å¿œã®æŠ¼ã—ã¦ãŠã...?
 * @param req
 * @returns
 */
export async function POST(req: Request) {
  try {
    const body = await req.json();
    const messages = body.messages ?? [];
    const sessionId = body.sessionId;
    if (!sessionId) {
      console.error("ğŸ’¿ memory API POST error: " + SESSIONID_ERROR);
      return Response.json({ error: SESSIONID_ERROR }, { status: 400 });
    }

    // 2è¡Œå–å¾—
    const len = messages.length;
    const previousMessage = messages.slice(Math.max(0, len - 2), len);

    // å±¥æ­´ç”¨ã‚­ãƒ¼
    const config = { configurable: { thread_id: sessionId } };
    const results = await app.invoke(
      { messages: previousMessage, sessionId: sessionId },
      config
    );

    return Response.json(results.formatted, { status: 200 });
  } catch (error) {
    const message = error instanceof Error ? error.message : UNKNOWN_ERROR;

    console.error("ğŸ’¿ memory API POST error: " + message);
    return Response.json({ error: message }, { status: 500 });
  }
}
